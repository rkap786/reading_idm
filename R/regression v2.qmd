---
title: "regression"
format: html
editor: visual
---

```{r}
library(readr)
library(tidyverse)
library(dplyr)
library(caret)
library(glmnet)
setwd("/Users/radhika/Library/CloudStorage/GoogleDrive-rkap786@stanford.edu/My Drive/0. Projects - Stanford/Item generation/")
source("Code/reading_idm/R/getAbility.R")
source("Code/reading_idm/R/Relasticnet.R")

```

Function for ridge regression

```{r}



```

### Data processing

Create file for analysis, fix p-values

```{r}

#setwd("G:/My Drive/0. Projects - Stanford/Item generation")
f= read_csv("Data/Data/Processed/data_qual_info.csv")
# glimpse(f)

f= f |> dplyr::select(-pdfname,-starts_with("PerOption"), 
                      -CorrectOptionNum,
                      -QuestionNo) #-DESWC

#f= f |> filter(pVal>0)
f= f |> mutate(pVal= ifelse(pVal==0, pVal+0.001, pVal))

### Average p-values by grade level based on NWEA & Texas
pval.state.grade= f |> group_by(grade, state) |> summarize(mean.pv= mean(pVal)) |> 
  pivot_wider(names_from = state,
              values_from=mean.pv)


pval.ny= as.vector(pval.state.grade[,2]$NY)
pval.tx= as.vector(pval.state.grade[,3]$Texas)

tx_ability=c(1467, 1552, 1592, 1634, 1669, 1698) #meets grade level performance
nwea_ny_ability=c(200.74, 204.83, 210.98, 215.36, 216.81, 220.93)
#vg= c(0, 0.6921, 0.6641, 1.4135, 1.2939, 1.9002)
grlist=sort(unique(f$grade))

### Convert ability to theta
nwea_ny_ability= (nwea_ny_ability-200)/10 ## following scale conversion
tx_ability = (tx_ability-1398.5930)/143.7195


### IRT
# b.ny=getAbility(pv=pval.ny, th.scale = nwea_ny_ability) ## This gives average difficulty of items by grade. This is what the pvalues should be
# ## We get the constant for adjustment
# adj= pval.ny - b.ny
# ny.adj.mat= data.frame(bind_cols(grade=grlist, adj.irt=adj, state= rep("NY",6)))
# 
# 
# b.tx=getAbility(pv=pval.tx, th.scale = tx_ability)
# 
# ## We get the constant for adjustment
# adj= pval.tx - b.tx
# tx.adj.mat= data.frame(bind_cols(grade=grlist, adj.irt=adj, state= rep("Texas",6)))
# 
# ## Adjusted difficulty for Texas
# adj.states = bind_rows(ny.adj.mat, tx.adj.mat)
# 
# 
# f= left_join(f, adj.states)
# f$pVal.irt = f$pVal - f$adj.irt
# f =f |> dplyr::select(-adj.irt)
# f |> group_by(state, grade) |> summarize(mean(pVal), mean(pVal.irt))
### mean difficulties are adjusted by grade

### Get pval IRT directly for each item
# 
# nwea.ability.mat= data.frame(bind_cols(grade=grlist, nwea.ability=nwea_ny_ability))
# f= left_join(f, nwea.ability.mat)

#### scale pvalue to range between 0.3 & 0.7
ny.pval.new= c(0.30, 0.40, 0.52, 0.59, 0.63,0.70)
ny.adj.c = pval.ny-ny.pval.new
tx.pval.new= c(0.30, 0.45, 0.52, 0.59, 0.65, 0.70)
tx.adj.c = pval.tx-tx.pval.new
ny.adj.mat= data.frame(bind_cols(grade=grlist, adj.cons=ny.adj.c, state= rep("NY",6)))
tx.adj.mat= data.frame(bind_cols(grade=grlist, adj.cons=tx.adj.c, state= rep("Texas",6)))
adj.states = bind_rows(ny.adj.mat, tx.adj.mat)

getlineardiff(tx_ability,f, state="Texas")

f= left_join(f, adj.states)
f$pVal.cons= f$pVal - f$adj.cons
f =f |> dplyr::select(-adj.cons)
# 
# f |> filter(state=="Texas", year==2022) |> group_by(state, grade, year) |> 
#   summarize(mean(pVal), 
#             mean(pVal.cons),
#             mean(pVal.irt, na.rm=T))
# 

#f= f |> na.omit()
```

Process data - set binary variables as factors

```{r}
f.names = names(f)

vars.yn= f.names[grepl("yn", f.names)]
for (vars in c(vars.yn, "year")) {
  f[[vars]]= as.factor(f[[vars]])
}



## Remove variables we dont need for analysis
f =f |>
  select(-pass_text_underline_yn,-pass_text_italics_yn,
                        -pass_text_underline_yn, -pass_text_bold_yn, -pass_text_fn_yn,-numPara, -PassNumUnq, -Passage, -starts_with("option_"), -QuestionText,-QNoUnq)

#f= f |> na.omit()


# vars.option= f.names[grepl("option", f.names)]
# vars.passage= c("Passage", "PassNumUnq", "year", "grade", "state", "ques_order",
#                 "QuestionText")
# vars.pval= f.names[grepl("pVal", f.names)]
# 
# f.indep= f[ , !(names(f) %in% c(vars.yn, vars.option, vars.passage, vars.pval))]
# f.dep =f[ , (names(f) %in% c(vars.yn, vars.option, vars.passage, vars.pval))]
# 
# 
# m = preProcess(f.indep, method=c("center", "scale"))
# f.indep.scale= predict(m, f.indep)
# f= bind_cols(f.dep, f.indep.scale)


```

Load file

```{r}




#f=   f |> select(pVal, PassageWordCount.gen, numPara, CorrectOptionNum, ques_order, numPara,BoxTextQuestion2)


## these cannot be continous numbers
#f$ques_order= as.factor(f$ques_order)
#f$CorrectOptionNum= as.factor(f$CorrectOptionNum)


#f.with.embed= f.with.embed |> select(-starts_with("id"))

# f.with.embed= f.with.embed |> select(-starts_with("id"))
# 
# f_cosine_diff= read_csv("Data/sim_corr_distractors_only.csv")
# names(f_cosine_diff)= c("diff_corr1","diff_corr2","diff_corr3")
# f.with.embed= bind_cols(f.with.embed,f_cosine_diff)


# f_correct_embed= read_csv("Data/question_embed_correct.csv")
# f_option1_embed= read_csv("Data/question_embed_dis1.csv")
# f_option2_embed= read_csv("Data/question_embed_dis2.csv")
# f_option3_embed= read_csv("Data/question_embed_dis3.csv")
# 
# names(f_correct_embed)= c("id",paste0("correct_embed", 1:769))
# names(f_option1_embed)= c("id",paste0("dis1_embed", 1:769))
# names(f_option2_embed)= c("id",paste0("dis2_embed", 1:769))
# names(f_option3_embed)= c("id",paste0("dis3_embed", 1:769))
# #names(f_option4_embed)= c("id",paste0("option4_embed", 1:769))
# 
# f.with.embed= bind_cols(f,f_correct_embed)
# f.with.embed= bind_cols(f.with.embed,f_option1_embed)
# f.with.embed= bind_cols(f.with.embed,f_option2_embed)
# f.with.embed= bind_cols(f.with.embed,f_option3_embed)
# #f.with.embed= bind_cols(f.with.embed,f_option4_embed)
# 


```

#### pvalue IRT coming from NWEA for NY and Texas for Texas 

```{r}

### Get pval IRT directly for each item

nwea.ability.mat= data.frame(bind_cols(grade=grlist, state=rep("NY", 6),ability=nwea_ny_ability))
tx_ability_mat= data.frame(bind_cols(grade=grlist, state=rep("Texas", 6),ability=tx_ability)) 
ability.mat= bind_rows(nwea.ability.mat,tx_ability_mat)

names(f)
#f= f |> select(-pVal.irt, -nwea.ability)
f= left_join(f, ability.mat)
f$pVal.irt = getlogitdiff(pv=f$pVal, th.scale = f$ability)

f= f |> dplyr::select(-ability)
#



```

Merge with embeddings

```{r}


f_all_options= read_csv("Data/Embeddings/question_alldistractors_embed.csv")
names(f_all_options)= c("id",paste0("embed.all.dis", 1:768))
f_all_options = f_all_options |> select(-id)

f.with.embed= bind_cols(f,f_all_options)
dim(f.with.embed)
dim(f_all_options)
names(f.with.embed)

### PCA embeddings
#embed= f.with.embed |> dplyr::select(starts_with("embed"))
pca_embeddings <- prcomp(f_all_options, center = TRUE, scale = TRUE) #
explained_variance <- summary(pca_embeddings)$importance[3, ]
num_components <- which(cumsum(explained_variance) >= 0.80)[1]
pca_embeddings_80 <- as.data.frame(pca_embeddings$x[, 1:num_components])
embed.pca.vars= names(pca_embeddings_80)

f.with.embed.pca= bind_cols(f,pca_embeddings_80)
dim(f.with.embed.pca)
names(f.with.embed.pca)

#### Without passage

f.embed.correct= read_csv("Data/Embeddings/option_embed_correct_tag.csv")
names(f.embed.correct)= c("id",paste0("embed.ques.cor", 1:768))

f.embed.optionembed1.tag= read_csv("Data/Embeddings/optionembed1_tag.csv")
names(f.embed.optionembed1.tag)= c("id",paste0("embed.ques.dis1", 1:768))

f.embed.optionembed2.tag= read_csv("Data/Embeddings/optionembed2_tag.csv")
names(f.embed.optionembed2.tag)= c("id",paste0("embed.ques.dis2", 1:768))

f.embed.optionembed3.tag= read_csv("Data/Embeddings/optionembed3_tag.csv")
names(f.embed.optionembed3.tag)= c("id",paste0("embed.ques.dis3", 1:768))


f.embed.merged.tag = bind_cols(f.embed.correct, f.embed.optionembed1.tag, f.embed.optionembed2.tag, f.embed.optionembed3.tag) |> 
  as.data.frame() |>
  select(-contains("id"))


f.with.embed2= bind_cols(f,f.embed.merged.tag)
dim(f.with.embed2)


### Alternate PCA embeddings
pca_embeddings <- prcomp(f.embed.merged.tag, center = TRUE, scale = TRUE) #
explained_variance <- summary(pca_embeddings)$importance[3, ]
num_components <- which(cumsum(explained_variance) >= 0.80)[1]
pca_embeddings_80 <- as.data.frame(pca_embeddings$x[, 1:num_components])

f.with.embed.pca2= bind_cols(f,pca_embeddings_80)
dim(f.with.embed.pca2)
names(f.with.embed.pca2)
embed.pca2.vars= names(pca_embeddings_80)

```

### Add  embeddings

Generate three datasets: 1. Item charactersitics & language characteristics 2. Item characteristics only 3. Item charactersitics, embeddings & language characteristics

```{r}

### f.analysis, f.train, f.test - all vars except embeddings
### f.analysis2 - without language metrics
### f.analysis3 - everything
### f.analysis4 - only embeddings
### f.analysis.pca - PCA embeddings plus everything else

#### Variable names
### all embeddings
vars = names(f.with.embed)
embed.vars = vars[grep("embed", vars)]
## PCA embeddings: embed.pca2.vars, embed.pca.vars




######### Remove embeddings
set.seed(123)
library(tidymodels)
# f.with.embed= f.with.embed |> dplyr::select(-PassNumUnq, -Passage, -starts_with("option_"), -QuestionText,-QNoUnq) |> na.omit()

f.analysis = f.with.embed |> dplyr::select(-starts_with("embed.all"))
glimpse(f.analysis)

set.seed(123)
library(tidymodels)
library(caret)
fsplit= f.analysis |> initial_split(prop=0.8, strata = pVal)

f.train= training(fsplit)
f.test= testing(fsplit)
dim(f.train)
dim(f.test)

#x2 = model.matrix(pVal~., f.train2)[,-1]
#y2= f.train2$pVal
#x.test2= model.matrix(pVal~., f.test2)[,-1]

##### Without language metrics
f.analysis2= f.analysis |> 
  dplyr::select(starts_with("pass"), starts_with("Pass"), ,starts_with("ques"),
         starts_with("option"), starts_with("num"), starts_with("pVal"), 
         year, state, grade)  |>
  dplyr::select(-PassageWordCount.gen)
         
fsplit2= f.analysis2 |> initial_split(prop=0.8, strata = pVal)
f.train2= training(fsplit2)
f.test2= testing(fsplit2)
dim(f.train2)
dim(f.test2)         

#### Item, language characteristics + Embeddings
f.analysis3= f.with.embed 
fsplit3= f.analysis3 |> initial_split(prop=0.8, strata = pVal)

f.train3= training(fsplit3)
f.test3= testing(fsplit3)
dim(f.train3)
dim(f.test3)    

#### Item, language characteristics + PCA Embeddings
f.analysis3.pca= f.with.embed.pca
         
fsplit3.pca= f.analysis3.pca |> initial_split(prop=0.8, strata = pVal)

f.train3.pca= training(fsplit3.pca)
f.test3.pca= testing(fsplit3.pca)
dim(f.train3.pca)
dim(f.test3.pca)   

### Only embeddings and item characteristics
f.analysis4=f.with.embed |> 
  dplyr::select(starts_with("pass"), starts_with("Pass"), ,starts_with("ques"),
         starts_with("option"), starts_with("num"), starts_with("pVal"), year, state, grade, starts_with("embed.all")) |> na.omit() |>
  dplyr::select(-PassageWordCount.gen)
 
         
fsplit4= f.analysis4 |> initial_split(prop=0.8, strata = pVal)

f.train4 =training(fsplit4)
f.test4= testing(fsplit4)
dim(f.train4)
dim(f.test4)         


### PCA embeddings plus everything else
dim(f_all_options)
#f.analysis.pca= bind_cols(f.analysis,pca_embeddings_80)
fsplit.pca= f.with.embed.pca |> initial_split(prop=0.8, strata = pVal)

f.train.pca =training(fsplit.pca)
f.test.pca= testing(fsplit.pca)
dim(f.train.pca)
dim(f.test.pca)     



### PCA embeddings option 2 plus everything else
dim(f_all_options)
#f.analysis.pca= bind_cols(f.analysis,pca_embeddings_80)
fsplit.pca2= f.with.embed.pca2 |> initial_split(prop=0.8, strata = pVal)

f.train.pca2 =training(fsplit.pca2)
f.test.pca2= testing(fsplit.pca2)
dim(f.train.pca2)
dim(f.test.pca2)     



```

# Ridge regression

## All variables except embeddings

Turn average correct into easy, medium, hard buckets

```{r}



g1= f |> dplyr::select(grade, starts_with("pVal")) |>
  ggplot(aes(grade, pVal.irt, fill=factor(grade), color=factor(grade))) + geom_boxplot(alpha=0.3) +
  scale_fill_brewer(palette = "RdBu") + 
  labs(x= "", y= "pValue adjusted to logit scale") + 
  theme_classic() +
  theme(legend.position = "bottom", legend.title=element_blank())

g2= f |> dplyr::select(grade, starts_with("pVal")) |>
  ggplot(aes(grade, pVal.cons, fill=factor(grade), color=factor(grade))) + geom_boxplot(alpha=0.3) +
  scale_fill_brewer(palette = "RdBu") + 
  labs(x= "", y= "pValue adjusted between 0.3 & 0.7") + 
  theme_classic() +
  theme(legend.position = "bottom", legend.title=element_blank())

g3= f |> dplyr::select(grade, starts_with("pVal")) |>
  ggplot(aes(grade, pVal, fill=factor(grade), color=factor(grade))) + geom_boxplot(alpha=0.3) +
  scale_fill_brewer(palette = "RdBu") + 
  labs(x= "", y= "pValue unadjusted") + 
  theme_classic() +
  theme(legend.position = "bottom", legend.title=element_blank())

library(ggpubr)
g= ggarrange(g3, g2, g1, nrow=1, common.legend = TRUE, align = "hv")
ggsave("Code/reading_idm/R/plots/pvalbygrade.png", g, width = 14, height = 8)
# 
# f |> group_by(grade) |>
#   summarise(meanfk= mean(fk, na.rm=T)) 


```

```{r}
### f.analysis, f.train, f.test - all vars except embeddings
### f.analysis2 - without language metrics
### f.analysis3 - everything
### f.analysis4 - only embeddings
### f.analysis.pca - PCA embeddings plus everything else
library(caret)

##
train= f.train2 |> select(-pVal.irt, -pVal.cons)
test= f.test2 |> select(-pVal.irt, -pVal.cons)
res_item= fnc_reg_elasticNet(train, test) # without language metrics


train= f.train |> select(-pVal.irt, -pVal.cons) |> na.omit()
test= f.test |> select(-pVal.irt, -pVal.cons) |> na.omit()
res_lang_item= fnc_reg_elasticNet(train, test) ## everything except embedding

train= f.train4 |> select(-pVal.irt, -pVal.cons) |> na.omit()
test= f.test4 |> select(-pVal.irt, -pVal.cons) |> na.omit()
res_item_embed= fnc_reg_elasticNet(train, test) ## embeddings only

train= f.train3 |> select(-pVal.irt, -pVal.cons) |> na.omit()
test= f.test3 |> select(-pVal.irt, -pVal.cons) |> na.omit()
res_item_all= fnc_reg_elasticNet(train, test) ## everything

train= f.train.pca |> select(-pVal.irt, -pVal.cons) |> na.omit()
test= f.test.pca |> select(-pVal.irt, -pVal.cons) |> na.omit()
res_lang_item_pcaembed= fnc_reg_elasticNet(train, test) ## everything


train= f.train.pca2 |> select(-pVal.irt, -pVal.cons) |> na.omit()
test= f.test.pca2 |> select(-pVal.irt, -pVal.cons) |> na.omit()
res_lang_item_pcaembed2= fnc_reg_elasticNet(train, test) ## everything


res_unadj= bind_rows(res_item, res_lang_item, res_item_embed, res_item_all, res_lang_item_pcaembed)
res_unadj= round(res_unadj,2)
res_unadj= bind_cols(type=rep("Unadjusted",nrow(res_unadj)),model=c("Only item descriptives", "Item descriptives & text analysis metrics", 
"Item descriptives & BERT embeddings", "Item descriptions, text analysis metrics, & BERT embeddings", "Item descriptions, text analysis metrics, & PCA on BERT embeddings"), res_unadj)

res_pca_results= bind_rows(res_lang_item_pcaembed, res_lang_item_pcaembed2)

### IRT pvalues
train= f.train2 |> select(-pVal, -pVal.cons)
test= f.test2 |> select(-pVal, -pVal.cons)
res_item= fnc_reg_elasticNet_irt(train, test) # without language metrics

train= f.train |> select(-pVal, -pVal.cons)
test= f.test |> select(-pVal, -pVal.cons)
res_lang_item= fnc_reg_elasticNet_irt(train, test) ## everything except embedding

train= f.train4 |> select(-pVal, -pVal.cons)
test= f.test4 |> select(-pVal, -pVal.cons)
res_item_embed= fnc_reg_elasticNet_irt(train, test) ## embeddings only

train= f.train3 |> select(-pVal, -pVal.cons)
test= f.test3 |> select(-pVal, -pVal.cons)
res_item_all= fnc_reg_elasticNet_irt(train, test) ## everything

train= f.train.pca |> select(-pVal, -pVal.cons)
test= f.test.pca |> select(-pVal, -pVal.cons)
res_lang_item_pcaembed= fnc_reg_elasticNet_irt(train, test) ## everything


res_irt= bind_rows(res_item, res_lang_item, res_item_embed, res_item_all, res_lang_item_pcaembed)

res_irt= round(res_irt,2)
res_irt= bind_cols(type=rep("IRT",nrow(res_irt)),model=c("Only item descriptives", "Item descriptives & text analysis metrics", 
"Item descriptives & BERT embeddings", "Item descriptions, text analysis metrics, & BERT embeddings", "Item descriptions, text analysis metrics, & PCA on BERT embeddings"), res_irt)

### Constant adjustment
train= f.train2 |> select(-pVal, -pVal.irt)
test= f.test2 |> select(-pVal, -pVal.irt)
res_item= fnc_reg_elasticNet_linear(train, test) # without language metrics

train= f.train |> select(-pVal, -pVal.irt)
test= f.test |> select(-pVal, -pVal.irt)
res_lang_item= fnc_reg_elasticNet_linear(train, test) ## everything except embedding

train= f.train4 |> select(-pVal, -pVal.irt)
test= f.test4 |> select(-pVal, -pVal.irt)
res_item_embed= fnc_reg_elasticNet_linear(train, test) ## embeddings only

train= f.train3 |> select(-pVal, -pVal.irt)
test= f.test3 |> select(-pVal, -pVal.irt)
res_item_all= fnc_reg_elasticNet_linear(train, test) ## everything


train= f.train.pca |> select(-pVal, -pVal.irt)
test= f.test.pca |> select(-pVal, -pVal.irt)
res_lang_item_pcaembed= fnc_reg_elasticNet_linear(train, test) ## everything


res_cons= bind_rows(res_item, res_lang_item, res_item_embed, res_item_all,
res_lang_item_pcaembed)
res_cons=round(res_cons,2)

res_cons= bind_cols(type=rep("Cons",nrow(res_cons)),model=c("Only item descriptives", "Item descriptives & text analysis metrics", 
"Item descriptives & BERT embeddings", "Item descriptions, text analysis metrics, & BERT embeddings", "Item descriptions, text analysis metrics, & PCA on BERT embeddings"), res_cons)
#View(res_cons)
kable(res_unadj[,-1], format="latex")
kable(res_cons[,-1], format="latex")
kable(res_irt[,-1], format="latex")
#bind_rows(res_unadj, res_cons, res_irt) |> View()

```

### PCA on embeddings

Using caret â€“ results are similar to using PCA directly

```{r}

# embedding_cols <- grep("embed.all", names(f.train3), value = TRUE)
# preprocess_embeddings <- preProcess(f.train3[, embedding_cols], method = c("pca"),
#                                     thresh=0.8)
# train_embeddings_pca <- predict(preprocess_embeddings, f.train3[, embedding_cols])
# test_embeddings_pca <- predict(preprocess_embeddings, f.test3[, embedding_cols])
# 
# train_no_embeddings <- f.train3[, !names(f.train3) %in% embedding_cols]
# test_no_embeddings <- f.test3[, !names(f.test3) %in% embedding_cols]
# 
# train_pca <- cbind(train_no_embeddings, train_embeddings_pca)
# test_pca <- cbind(test_no_embeddings, test_embeddings_pca)
# 
# res_item_all_pca= fnc_reg_elasticNet(train_pca, test_pca, alpha=0) ## everything
# res_item_all_pca
# res_item_all
# 
# 
# res_item_all_pca= fnc_reg_elasticNet_linear(train_pca, test_pca, alpha=0) ## everything
# res_item_all_pca
# res_item_all
# 
# 
# res_item_all_pca= fnc_reg_elasticNet_irt(train_pca, test_pca, alpha=0) ## everything
# res_item_all_pca
# res_item_all

```

### Check if we need to eliminate linguistic features

```{r}


# # Define lambda range for ridge regression
# lambda_seq <- seq(0, 10, length = 50)
# 
# # Set up training control for cross-validation
# train_control <- trainControl(method = "cv", number = 10)
# 
# # Define ridge regression model using traditional features only
# ridge_model <- train(
#   pVal ~ ., 
#   data = f.train, 
#   method = "glmnet",
#   family = "gaussian",
#   trControl = train_control,
#   tuneGrid = expand.grid(alpha = 0, lambda = lambda_seq),
#   preProcess = c("center", "scale")
# )
# 
# # Apply RFE with ridge regression to select the best features
# rfe_control <- rfeControl(functions = caretFuncs, method = "cv", number = 10)
# 
# # Run RFE with a ridge model to find the best feature subset
# rfe_model <- rfe(
#   pVal ~ ., 
#   data = f.train, 
#   sizes = c(5, 10, 15, 20, 25), # Number of features to test
#   rfeControl = rfe_control,
#   method = "glmnet",
#   family = "gaussian",
#   trControl = train_control,
#   tuneGrid = expand.grid(alpha = 0, lambda = lambda_seq),
#   preProcess = c("center", "scale")
# )

# # View selected features
# print(rfe_model$optVariables)
# ridge_model_final$bestTune
# 
# # Train final ridge model with the selected features
# selected_features <- rfe_model$optVariables
# ridge_model_final <- train(
#   pVal ~ ., 
#   data = f.train[, c(selected_features, "pVal")],
#   method = "glmnet",
#   family = "gaussian",
#   trControl = train_control,
#   tuneGrid = expand.grid(alpha = 0, lambda = lambda_seq),
#   preProcess = c("center", "scale")
# )
# 
# # Evaluate final model
# ridge_model_final$bestTune
# ridge_model_final$results |> 
#   mutate(lambda = log(lambda)) |>
#   ggplot(aes(x = lambda, y = RMSE, col = factor(alpha))) +
#   geom_line() + geom_point() + theme_minimal()
# 
# # Predictions on test data using selected features
# y_test_hat <- predict(ridge_model_final, test[, selected_features])
# y_train_hat <- predict(ridge_model_final, train[, selected_features])
# 
# # Performance evaluation
# print(root_mean_squared_error(y_test, y_test_hat))
# print(cor(y_test, y_test_hat))




```

### Random forest

```{r}
# library(caret)
# fitControl= trainControl(method= "cv", number=5,classProbs = TRUE)

```

```{r}
ptm <- proc.time()
library(ranger)
library(mlr)
library(caret)
# Choose resampling strategy and define grid


train= f.train.pca |> select(-pVal, -pVal.irt)
test= f.test.pca |> select(-pVal, -pVal.irt)

control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3)

#f.train.pca, f.test.pca
fit.rf= caret::train(
            pVal.cons~ .,
            data=train,
              method='ranger',
              importance="permutation",
              metric= "RMSE" #num.trees = 100,
) #trControl = control,
              #tuneLength = 15,


# m.randomForest <- train(pVal ~ ., 
#                       data = f.train, 
#                       method = "rf", 
#                       trControl = fitControl,
#                       na.action = na.omit,
#                       trace = FALSE)

rfpred= predict(fit.rf, newdata=f.test,type="prob")

Metrics::rmse(f.test$pVal,rfpred)

hist(pred1)
hist(f.test$pVal)
cor(f.test$pVal, rfpred)





```

```{r}

ptm <- proc.time()
library(ranger)
library(mlr)
library(caret)
# Choose resampling strategy and define grid
ptm <- proc.time()

train= f.train.pca |> select(-pVal, -pVal.irt)
test= f.test.pca |> select(-pVal, -pVal.irt)
n_features <- length(names(f.train.pca))-1


control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3)
# 
# h2o.no_progress()
# h2o.init(max_mem_size = "5g")
# train_h2o <- as.h2o(train)

hyper_grid <- expand.grid(
  mtry = floor(n_features * c(.15, .33, .5)),
  min.node.size = c(10, ), 
)

#f.train.pca, f.test.pca
for(i in seq_len(nrow(hyper_grid))) {
fit.rf= caret::train(
            pVal.cons~ .,
            data=train,
            method='ranger',
            num.trees = n_features * 10,
            importance="impurity",
            mtry=hyper_grid$mtry[i],
            min.node.size   = hyper_grid$min.node.size[i],
            replace         = hyper_grid$replace[i],
            sample.fraction = hyper_grid$sample.fraction[i],
            verbose         = FALSE,
            seed            = 123,
            respect.unordered.factors = 'order',
)
hyper_grid$rmse[i] <- sqrt(fit.rf$prediction.error)
}

p <- vip::vip(fit.rf, num_features = 25, bar = FALSE)
#p2 <- vip::vip(rf_permutation, num_features = 25, bar = FALSE)

#gridExtra::grid.arrange(p1, p2, nrow = 1)
# m.randomForest <- train(pVal ~ ., 
#                       data = f.train, 
#                       method = "rf", 
#                       trControl = fitControl,
#                       na.action = na.omit,
#                       trace = FALSE)

rfpred= predict(fit.rf, newdata=f.test,type="prob")

Metrics::rmse(f.test$pVal,rfpred)

hist(pred1)
hist(f.test$pVal)
cor(f.test$pVal, rfpred)

```

## Neural net

```{r}

fitControl= trainControl(method="cv",number=5)
tune.grid.neuralnet <- expand.grid(
  layer1 = c(3, 10, 20),
  layer2 = c(3,10,20),
  layer3 = c(1)
)
m.NeuralNet <- train(pVal ~ ., 
                      data = f.train, 
                      method = "neuralnet", 
                      trControl = fitControl,
                     preProc = c("center", "scale"),
                     metric = "RMSE",
                      na.action = na.omit,
                     tuneGrid=tune.grid.neuralnet
                    )


nnpred= predict(m.NeuralNet, newdata=f.test)

Metrics::rmse(f.test$pVal,nnpred)

hist(nnpred)
hist(f.test$pVal)
cor(f.test$pVal, nnpred)

```
